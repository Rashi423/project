{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "46HzLg6VounC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dbebd6-b5b4-4224-aac5-f00992ddd230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# /content/drive/MyDrive/Final Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set working directory (replace 'path/to/your/data' with your actual data path)\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/FinalProject')\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "\n",
        "# Print working directory for verification\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sg0EOdUUElq",
        "outputId": "0b29aaa4-a1e1-47fe-abf0-cbb7dc84a273"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/drive/MyDrive/FinalProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file=\"/content/drive/MyDrive/FinalProject/data/captions.txt\""
      ],
      "metadata": {
        "id": "xISTFGjPjpMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lywzrYsBuqWn",
        "outputId": "9896f181-d5a3-40cc-940e-f2f1eda7f2ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import ImageCaptionDataset\n",
        "from encoder import Encoder\n",
        "from decoder import DecoderWithAttention\n"
      ],
      "metadata": {
        "id": "sx495Xixuk_y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Convert captions.txt from CSV to tab-separated format\n",
        "input_file = \"data/captions.txt\"  # Update if necessary\n",
        "output_file = \"data/captions_fixed.txt\"  # Adjust output path as needed\n",
        "\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "    reader = csv.reader(infile)  # Default is comma-delimited\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        # Ensure the row has at least two columns\n",
        "        if len(row) >= 2:\n",
        "            img_id = row[0].strip()  # Image ID\n",
        "            caption = \",\".join(row[1:]).strip()  # Join remaining parts as the caption\n",
        "            outfile.write(f\"{img_id}\\t{caption}\\n\")\n",
        "\n",
        "print(f\"Converted captions saved to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqjEq40117Z",
        "outputId": "c6d3d83a-b6bd-40cc-8beb-a12c2c32d530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted captions saved to data/captions_fixed.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/captions_fixed.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    print(f\"Total lines: {len(lines)}\")\n",
        "    print(f\"First 5 lines: {lines[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA0AzgFX4inh",
        "outputId": "8bab61a5-3902-425c-d5a7-eeb6fe62b8fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines: 40455\n",
            "First 5 lines: ['1000268201_693b08cb0e.jpg\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n', '1000268201_693b08cb0e.jpg\\tA girl going into a wooden building .\\n', '1000268201_693b08cb0e.jpg\\tA little girl climbing into a wooden playhouse .\\n', '1000268201_693b08cb0e.jpg\\tA little girl climbing the stairs to her playhouse .\\n', '1000268201_693b08cb0e.jpg\\tA little girl in a pink dress going into a wooden cabin .\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import ImageCaptionDataset\n",
        "from torchvision import transforms\n",
        "\n",
        "dataset = ImageCaptionDataset(\n",
        "    \"data/images\",\n",
        "    \"data/captions_fixed.txt\",\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        ")\n"
      ],
      "metadata": {
        "id": "3-unFCxa5jSm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset size: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LbC30cZ6-25",
        "outputId": "df3c3ddc-f479-495d-cb6f-342078622164"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCpRc2_evFKI",
        "outputId": "459865d3-090f-4fa1-f35a-d4501e7a81fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FinalProject/train.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  captions = [torch.tensor(caption, dtype=torch.long) for caption in captions]\n",
            "Model checkpoints saved to data/encoder.pth and data/decoder.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGTABhB2ZUML",
        "outputId": "60903beb-e18f-4f42-c6d9-310cf3d16517"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FinalProject/evaluate.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  encoder.load_state_dict(torch.load(\"/content/drive/MyDrive/FinalProject/encoder.pth\"))\n",
            "/content/drive/MyDrive/FinalProject/evaluate.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(\"/content/drive/MyDrive/FinalProject/decoder.pth\"))\n",
            "Accuracy: 90.60%\n",
            "BLEU Score: 0.5545\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        <pad>       0.00      0.00      0.00         0\n",
            "         Dark       0.00      0.00      0.00         1\n",
            "        Blond       0.00      0.00      0.00         2\n",
            "            A       0.90      1.00      0.95      1148\n",
            "            a       0.97      1.00      0.98        86\n",
            "        Women       0.00      0.00      0.00         2\n",
            "           an       1.00      1.00      1.00         3\n",
            "        Light       0.00      0.00      0.00         1\n",
            "         girl       0.00      0.00      0.00         0\n",
            "   Bicyclists       0.00      0.00      0.00         2\n",
            "          the       0.00      0.00      0.00         0\n",
            "    Shirtless       0.00      0.00      0.00         1\n",
            "          Two       0.98      1.00      0.99       207\n",
            "         dogs       1.00      1.00      1.00         1\n",
            "          One       0.00      0.00      0.00        11\n",
            "        There       1.00      0.29      0.44        14\n",
            "        Young       1.00      0.95      0.97        19\n",
            "          man       1.00      1.00      1.00         1\n",
            "         Sled       0.00      0.00      0.00         1\n",
            "        While       0.00      0.00      0.00         1\n",
            "        Beach       0.00      0.00      0.00         1\n",
            "          The       0.88      1.00      0.93       156\n",
            "      smiling       1.00      1.00      1.00         1\n",
            "         Busy       0.00      0.00      0.00         1\n",
            "      Terrier       0.00      0.00      0.00         1\n",
            "      Parents       0.00      0.00      0.00         1\n",
            "         twin       0.00      0.00      0.00         1\n",
            "      closeup       0.00      0.00      0.00         1\n",
            "         Twin       0.00      0.00      0.00         1\n",
            "          Dog       0.00      0.00      0.00         3\n",
            "        White       0.00      0.00      0.00         5\n",
            "          one       1.00      1.00      1.00         2\n",
            "        Woman       1.00      1.00      1.00        13\n",
            "      Smiling       0.00      0.00      0.00         3\n",
            "         Dogs       0.00      0.00      0.00         3\n",
            "           On       0.00      0.00      0.00         2\n",
            "      Skiiers       0.00      0.00      0.00         2\n",
            "        Hiker       0.00      0.00      0.00         1\n",
            "       Family       0.00      0.00      0.00         1\n",
            "    Energetic       0.00      0.00      0.00         1\n",
            "       Taking       0.00      0.00      0.00         1\n",
            "          Man       1.00      1.00      1.00        20\n",
            "         Some       0.00      0.00      0.00         5\n",
            "         Many       0.00      0.00      0.00         1\n",
            "       people       1.00      1.00      1.00         1\n",
            "        Seven       0.00      0.00      0.00         2\n",
            "      Several       0.00      0.00      0.00         8\n",
            "        Large       0.00      0.00      0.00         1\n",
            "        Shaft       0.00      0.00      0.00         1\n",
            "       Vendor       0.00      0.00      0.00         1\n",
            "   Spelunkers       0.00      0.00      0.00         1\n",
            "  Firefighter       0.00      0.00      0.00         1\n",
            "          Tan       0.00      0.00      0.00         1\n",
            "       Little       0.00      0.00      0.00         8\n",
            "        Girls       0.00      0.00      0.00         3\n",
            "        These       0.00      0.00      0.00         3\n",
            "       Couple       0.00      0.00      0.00         1\n",
            "         Kids       0.00      0.00      0.00         1\n",
            "          Boy       0.00      0.00      0.00         5\n",
            "           At       0.00      0.00      0.00         1\n",
            "         This       0.00      0.00      0.00         5\n",
            "          men       1.00      1.00      1.00         1\n",
            "        Rider       0.00      0.00      0.00         1\n",
            "       Blurry       0.00      0.00      0.00         1\n",
            "           An       1.00      1.00      1.00        23\n",
            "          two       1.00      1.00      1.00        13\n",
            "      Climber       0.00      0.00      0.00         2\n",
            "        Small       0.00      0.00      0.00         3\n",
            "        Black       0.00      0.00      0.00         4\n",
            "         Girl       0.00      0.00      0.00        10\n",
            "       Hockey       0.00      0.00      0.00         1\n",
            "      Toddler       0.00      0.00      0.00         1\n",
            "        three       1.00      1.00      1.00         7\n",
            "        Three       0.77      1.00      0.87        75\n",
            "       Person       0.00      0.00      0.00         5\n",
            "       Behind       0.00      0.00      0.00         1\n",
            "         View       0.00      0.00      0.00         1\n",
            "         this       0.00      0.00      0.00         1\n",
            "       People       1.00      0.94      0.97        31\n",
            "        About       0.00      0.00      0.00         1\n",
            "    Beautiful       0.00      0.00      0.00         1\n",
            "         Baby       0.00      0.00      0.00         2\n",
            "Cross-country       0.00      0.00      0.00         1\n",
            "         Tree       0.00      0.00      0.00         1\n",
            "        Asian       0.00      0.00      0.00         1\n",
            "      Topless       0.00      0.00      0.00         1\n",
            "          Guy       0.00      0.00      0.00         1\n",
            "         Both       0.00      0.00      0.00         1\n",
            "      Someone       0.00      0.00      0.00         3\n",
            "        Crowd       0.00      0.00      0.00         1\n",
            "        Brown       0.00      0.00      0.00         4\n",
            "          Kid       0.00      0.00      0.00         1\n",
            "        girls       1.00      1.00      1.00         1\n",
            "      Dogsled       0.00      0.00      0.00         1\n",
            "         kids       1.00      1.00      1.00         1\n",
            "         Five       0.00      0.00      0.00         1\n",
            "     Children       1.00      0.09      0.17        11\n",
            "         Four       0.00      0.00      0.00         7\n",
            "         Grey       0.00      0.00      0.00         1\n",
            " Snowboarders       0.00      0.00      0.00         1\n",
            "     Standing       0.00      0.00      0.00         1\n",
            "        Child       0.00      0.00      0.00         3\n",
            "\n",
            "     accuracy                           0.91      2000\n",
            "    macro avg       0.22      0.21      0.21      2000\n",
            " weighted avg       0.84      0.91      0.87      2000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}